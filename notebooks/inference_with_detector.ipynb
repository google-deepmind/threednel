{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae995168",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 DeepMind Technologies Limited\n",
    "# Copyright 2023 Massachusetts Institute of Technology (M.I.T.)\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22fd4d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Estimating 6D object poses of YCB objects from arbitrary RGB-D images using the 3D Neural Embedding Likelihood (3DNEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1d347",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook illustrates how we can do pose estimation on arbitrary RGB-D images containing YCB objects using 3DNEL introduced in the ICCV 2023 paper: [3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation](https://arxiv.org/abs/2302.03744)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import taichi as ti\n",
    "import torch\n",
    "from threednel.bop.data import BOPTestDataset, RGBDImage\n",
    "from threednel.bop.detector import Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039caa6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Setup\n",
    "3DNEL makes effective use of GPUs through a combination of JAX, taichi and PyTorch for parallel likelihood evaluation.\n",
    "\n",
    "We first initialize taichi on the CUDA backend, and get the data directory from the environment variable. We then initialize a detector, which we can use to estimate the 6D object poses of YCB objects in arbitrary RGB-D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Taichi with CUDA backend\n",
    "ti.init(arch=ti.cuda)\n",
    "# Get data directory from the environment variable\n",
    "data_directory = os.environ[\"BOP_DATA_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27fdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a detector to do pose estimation on RGB-D images with YCB objects.\n",
    "detector = Detector(\n",
    "    data_directory=data_directory,\n",
    "    n_passes_pose_hypotheses=1,\n",
    "    n_passes_icp=1,\n",
    "    n_passes_finetune=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf443323",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Initialize an RGBDImage object using an input RGB-D image.\n",
    "Our detector takes as input an RGBDImage object. In this example, we use an input image from the [YCB-V dataset as part of the BOP Challenge](https://bop.felk.cvut.cz/datasets/) as an example. However, in general 3DNEL can be applied to arbitrary RGB-D image to estimate 6D object poses of the YCB objects in the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the BOPTestDataset object\n",
    "data = BOPTestDataset(\n",
    "    data_directory=data_directory,\n",
    "    load_detector_crops=True,\n",
    ")\n",
    "# Load the 1st image in scene 48\n",
    "scene_id = 48\n",
    "test_scene = data[scene_id]\n",
    "img_id = test_scene.img_indices[1]\n",
    "bop_img = test_scene[img_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e354398",
   "metadata": {},
   "source": [
    "Next we construct an RGBDImage object as input to our detector.\n",
    "An RGBDImage object can be constructed from an RGB-D image and known camera intrinsics.\n",
    "Pose estimation with 3DNEL assumes knowledge of the number of objects and object classes in the scene. These are specified using `bop_obj_indices`, which is an array with elements ranging from 1 to 21. Refer to the [YCB object models](https://bop.felk.cvut.cz/media/data/bop_datasets/ycbv_models.zip) for the object indices of different supported objects.\n",
    "Empirically we find that filling in missing values in the depth map helps with performance.\n",
    "3DNEL can optionally take 2D detection results as part of the annotations to help with pose hypotheses generation, although it works even without initial 2D detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc76967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an RGBDImage object from the BOP test dataset input image.\n",
    "test_img = RGBDImage(\n",
    "    rgb=bop_img.rgb,\n",
    "    depth=bop_img.depth,\n",
    "    intrinsics=bop_img.intrinsics,\n",
    "    bop_obj_indices=np.array(bop_img.bop_obj_indices),\n",
    "    fill_in_depth=True,  # Fill in missing values in the depth map\n",
    "    max_depth=1260.0,  # Used to fill in missing values in the depth map\n",
    "    annotations=bop_img.annotations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b925a3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Estimating 6D object poses and visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c5dba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# The detector has a simple interface for estimating 6D object poses\n",
    "scale_factor = 0.25\n",
    "detection_results = detector.detect(\n",
    "    img=test_img,\n",
    "    key=jax.random.PRNGKey(np.random.randint(0, 100000)),\n",
    "    scale_factor=scale_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ee2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the query embedding maps for different objects.\n",
    "fig, ax = plt.subplots(\n",
    "    1,\n",
    "    len(test_img.bop_obj_indices),\n",
    "    figsize=(10 * len(test_img.bop_obj_indices), 10),\n",
    ")\n",
    "for ii in range(len(test_img.bop_obj_indices)):\n",
    "  ax[ii].imshow(\n",
    "      detector.bop_surfemb.surfemb_model.get_emb_vis(\n",
    "          torch.from_numpy(jax.device_get(detection_results.query_embeddings)[:, :, ii])\n",
    "      )\n",
    "      .cpu()\n",
    "      .numpy()\n",
    "  )\n",
    "  ax[ii].axis('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c042926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the estimated 3D scene descriptions in terms of 6D object poses.\n",
    "gl_renderer = test_img.get_renderer(data_directory)\n",
    "rendered_data = gl_renderer.render_single(\n",
    "    detection_results.inferred_poses,\n",
    "    list(range(len(test_img.bop_obj_indices))),\n",
    ")\n",
    "gt_rendered_data = gl_renderer.render_single(\n",
    "    jnp.array(bop_img.get_gt_poses()),\n",
    "    list(range(len(test_img.bop_obj_indices))),\n",
    ")\n",
    "gt_depth = gt_rendered_data.model_xyz[..., -1]\n",
    "low = gt_depth.min()\n",
    "high = gt_depth.max()\n",
    "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
    "ax[0].imshow(test_img.rgb)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Input scene', fontsize=40)\n",
    "ax[1].imshow(gt_depth, cmap=\"turbo\", vmin=low, vmax=high)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Ground-truth 3D scene description', fontsize=40)\n",
    "ax[2].imshow(rendered_data.model_xyz[..., -1], cmap=\"turbo\", vmin=low, vmax=high)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title('Estimated 3D scene description', fontsize=40)\n",
    "ax[3].imshow(rendered_data.obj_ids)\n",
    "ax[2].axis('off')\n",
    "ax[3].set_title('Estimated object segmentation', fontsize=40)\n",
    "ax[3].axis('off')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
